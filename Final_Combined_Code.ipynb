{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "eiJk44hJ5xsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this program, users can put the text they would like to analyze into the variable input_text, and the program would output whether this text is offensive language or hate speech or neither of them, along with the category of this text is it is offensive language or hate speech."
      ],
      "metadata": {
        "id": "Fgf0ijkB8WIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input Text"
      ],
      "metadata": {
        "id": "3N1647Mc69vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input text\n",
        "# change to the input text needs to be analyzed\n",
        "# this example is a twitter style text on sports with a short offensive language does not have any specific topic\n",
        "input_text = \"Las Vegas Aces win their first WNBA title! Chelsea Gray MVP First pro sports champ for Vegas! get to hit any bitch he wants to\""
      ],
      "metadata": {
        "id": "4CwxDSeg5yA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model<br>"
      ],
      "metadata": {
        "id": "gYdBK_3q6vfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we would use the best model architecture of the hate speech detection and topic classfication through the experiments we did. Since the performance has already been evaluated during previous experiments (see seperated file for each model experiments and their evaluation), we would use the whole dataset for training instead of train-test-split."
      ],
      "metadata": {
        "id": "R8xuKRIJ7QIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras as kr\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
        "from keras.layers import SimpleRNN, Bidirectional, MaxPooling1D, GlobalMaxPool1DGRU\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers"
      ],
      "metadata": {
        "id": "heg3TC70mJHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "#visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "#consistent sized plot\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize']=12,5\n",
        "rcParams['axes.labelsize']=12\n",
        "rcParams['xtick.labelsize']=12\n",
        "rcParams['ytick.labelsize']=12"
      ],
      "metadata": {
        "id": "EuiJruJN6kwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
        "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
        "#text preprocessing libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkCNQrmT7_DJ",
        "outputId": "72cbf78b-745e-4d4b-e964-3b4ecbd55c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#variable to record the output from two models\n",
        "hate_speech_classified_output = None\n",
        "text_category_output = None"
      ],
      "metadata": {
        "id": "vFR0m52VIS7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train hate speech dection model\n",
        "tweet = pd.read_csv('hate_speech_labeled_data.csv',delimiter=',',engine='python',encoding='utf-8-sig')\n",
        "c=tweet['class']\n",
        "tweet.rename(columns={'tweet' : 'text',\n",
        "                   'class' : 'category'},\n",
        "                    inplace=True)\n",
        "a=tweet['text']\n",
        "b=tweet['category'].map({0: 'hate_speech', 1: 'offensive_language',2: 'neither'})\n",
        "\n",
        "df= pd.concat([a,b,c], axis=1)\n",
        "df.rename(columns={'class' : 'label'},\n",
        "                    inplace=True)\n",
        "\n",
        "# Handle Diacritics with text normalization\n",
        "def simplify(text):\n",
        "    '''Function to handle the diacritics in the text'''\n",
        "    import unicodedata\n",
        "    try:\n",
        "        text = unicode(text, 'utf-8')\n",
        "    except NameError:\n",
        "        pass\n",
        "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
        "    return str(text)\n",
        "\n",
        "df['text'] = df['text'].apply(simplify)\n",
        "# Remove user handles\n",
        "df['text'].replace(r'@\\w+','',regex=True,inplace=True)\n",
        "# Remove the urls\n",
        "df['text'].replace(r'http\\S+','',regex=True,inplace=True)\n",
        "tokenizer = TweetTokenizer(preserve_case=True)\n",
        "df['text'] = df['text'].apply(tokenizer.tokenize)\n",
        "# Remove Stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "#add additional stop words to be removed from the text\n",
        "additional_list = ['amp','rt','u',\"can't\",'ur']\n",
        "\n",
        "for words in additional_list:\n",
        "    stop_words.append(words)\n",
        "\n",
        "#remove stop words\n",
        "def remove_stopwords(text):\n",
        "    '''Function to remove the stop words from the text corpus'''\n",
        "    clean_text = [word for word in text if not word in stop_words]\n",
        "    return clean_text\n",
        "df['text'] = df['text'].apply(remove_stopwords)\n",
        "\n",
        "# Spelling corrections\n",
        "from textblob import TextBlob\n",
        "\n",
        "def spell_check(text):\n",
        "    '''Function to do spelling correction using '''\n",
        "    txtblob = TextBlob(text)\n",
        "    corrected_text = txtblob.correct()\n",
        "    return corrected_text\n",
        "\n",
        "# remove hashtags\n",
        "def remove_hashsymbols(text):\n",
        "    pattern = re.compile(r'#')\n",
        "    text = ' '.join(text)\n",
        "    clean_text = re.sub(pattern,'',text)\n",
        "    return tokenizer.tokenize(clean_text)\n",
        "df['text'] = df['text'].apply(remove_hashsymbols)\n",
        "\n",
        "# remove short words\n",
        "def rem_shortwords(text):\n",
        "    lengths = [1, 2]\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if len(word) not in lengths]\n",
        "    new_text = ' '.join(filtered_tokens)\n",
        "    return new_text\n",
        "df['text'] = df['text'].apply(rem_shortwords)\n",
        "\n",
        "# tokenization\n",
        "df['text'] = df['text'].apply(tokenizer.tokenize)\n",
        "\n",
        "# Remove digits\n",
        "def rem_digits(text):\n",
        "    '''Function to remove the digits from the list of strings'''\n",
        "    no_digits = []\n",
        "    for word in text:\n",
        "        no_digits.append(re.sub(r'\\d','',word))\n",
        "    return ' '.join(no_digits)\n",
        "df['text'] = df['text'].apply(rem_digits)\n",
        "df['text'] = df['text'].apply(tokenizer.tokenize)\n",
        "\n",
        "# Remove special characters\n",
        "def rem_nonalpha(text):\n",
        "    '''Function to remove the non-alphanumeric characters from the text'''\n",
        "    text = [word for word in text if word.isalpha()]\n",
        "    return text\n",
        "\n",
        "# remove the non alpha numeric characters from the tweet tokens\n",
        "df['text'] = df['text'].apply(rem_nonalpha)\n",
        "\n",
        "# TfidfVectorizer\n",
        "df1 = df.copy()\n",
        "df1['text'] = df1['text'].apply(lambda x: ' '.join(x))\n",
        "X_train = df1['text']\n",
        "y_train = df1['label']\n",
        "vectorizer = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "hate_speech_classifier = LogisticRegression()\n",
        "hate_speech_classifier.fit(X_train_vec,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "b6incZhA8jQk",
        "outputId": "68b32876-c476-40ec-810a-ae12c43b3b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_base_model = transformers.TFAutoModel.from_pretrained('bert-large-uncased')\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "def load_and_preprocess_data(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    print(df.head())\n",
        "    return df\n",
        "def tokenize_texts(text, tokenizer, max_len):\n",
        "    text = text.tolist()\n",
        "    tokenized_output = tokenizer(\n",
        "        text = text,\n",
        "        padding='max_length',\n",
        "        max_length=max_len,\n",
        "        truncation=True,\n",
        "        return_tensors='np',\n",
        "        is_split_into_words=False,\n",
        "        stride = 0,\n",
        "        return_overflowing_tokens=False,\n",
        "    )\n",
        "    tokenized_input_ids = tokenized_output['input_ids']\n",
        "    return tokenized_input_ids\n",
        "def split_data(df, test_size=0.2, random_state=42):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['article'], df['category'], test_size=test_size, random_state=random_state)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "def one_hot_encode(labels, num_classes=10):\n",
        "    one_hot_encoded_label = tf.keras.utils.to_categorical(labels, num_classes=num_classes, dtype='int32')\n",
        "    return one_hot_encoded_label\n",
        "df = load_and_preprocess_data('data_preprocessed.csv')\n",
        "\n",
        "X_train, X_test, y_train, y_test = split_data(df, test_size=0.2, random_state=42)\n",
        "\n",
        "max_len = 60\n",
        "tokenizer_x_train = tokenizer(\n",
        "        text = X_train.astype('str').tolist(),\n",
        "        padding='max_length',\n",
        "        max_length=max_len,\n",
        "        truncation=True,\n",
        "        return_tensors='np',\n",
        "        is_split_into_words=False,\n",
        "        stride = 0,\n",
        "        return_overflowing_tokens=False,\n",
        "    )\n",
        "X_train_tokenized = tokenizer_x_train['input_ids']\n",
        "\n",
        "tokenizer_x_test = tokenizer(\n",
        "        text = X_test.astype('str').tolist(),\n",
        "        padding='max_length',\n",
        "        max_length=max_len,\n",
        "        truncation=True,\n",
        "        return_tensors='np',\n",
        "        is_split_into_words=False,\n",
        "        stride = 0,\n",
        "        return_overflowing_tokens=False,\n",
        "    )\n",
        "X_test_tokenized = tokenizer_x_test['input_ids']\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "y_train = one_hot_encode(encoder.fit_transform(y_train))\n",
        "y_test = one_hot_encode(encoder.transform(y_test))\n",
        "\n",
        "\n",
        "def construct_bert_based_classifier(bert_layer, total_classes=10):\n",
        "\n",
        "    bert_input = tf.keras.layers.Input(shape=(60,), dtype=tf.int32, name='bert_input_layer')\n",
        "    dropout_seq = tf.keras.layers.Dropout(0.3, name='dropout_for_generalization')(bert_layer(bert_input)[0][:, 0, :])\n",
        "    prediction_layer = tf.keras.layers.Dense(total_classes, activation='softmax', name='prediction_layer')(dropout_seq)\n",
        "    bert_classifier = tf.keras.Model(inputs=bert_input, outputs=prediction_layer)\n",
        "    bert_classifier.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
        "                            loss='categorical_crossentropy',\n",
        "                            metrics=['accuracy'])\n",
        "\n",
        "    return bert_classifier\n",
        "\n",
        "text_classification_model = construct_bert_based_classifier(bert_base_model)\n",
        "\n",
        "text_classification_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvaIv5MkjcwW",
        "outputId": "95604a42-946a-483c-838d-e7a504827d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        category                                            article\n",
            "0         COMEDY  of the funniest tweets about cats and dogs thi...\n",
            "1      PARENTING  the funniest tweets from parents this week sep...\n",
            "2         SPORTS  maury wills basestealing shortstop for dodgers...\n",
            "3  ENTERTAINMENT  golden globes returning to nbc in january afte...\n",
            "4       POLITICS  biden says us forces would defend taiwan if ch...\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 60)]              0         \n",
            "                                                                 \n",
            " tf_bert_model_3 (TFBertMod  TFBaseModelOutputWithPo   335141888 \n",
            " el)                         olingAndCrossAttentions             \n",
            "                             (last_hidden_state=(Non             \n",
            "                             e, 60, 1024),                       \n",
            "                              pooler_output=(None, 1             \n",
            "                             024),                               \n",
            "                              past_key_values=None,              \n",
            "                             hidden_states=None, att             \n",
            "                             entions=None, cross_att             \n",
            "                             entions=None)                       \n",
            "                                                                 \n",
            " tf.__operators__.getitem (  (None, 1024)              0         \n",
            " SlicingOpLambda)                                                \n",
            "                                                                 \n",
            " dropout_292 (Dropout)       (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 335152138 (1.25 GB)\n",
            "Trainable params: 335152138 (1.25 GB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((X_train_tokenized, y_train)).repeat().shuffle(2048).batch(BATCH_SIZE).prefetch(AUTO)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((X_test_tokenized, y_test)).batch(BATCH_SIZE)\n",
        "\n",
        "steps_per_epoch = 300\n",
        "training_history = text_classification_model.fit(train_data, steps_per_epoch=steps_per_epoch, epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkySFJMomWPK",
        "outputId": "504b7429-43e8-474c-b1c4-a09bb32205c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "300/300 [==============================] - 369s 1s/step - loss: 0.6905 - accuracy: 0.7860\n",
            "Epoch 2/5\n",
            "300/300 [==============================] - 368s 1s/step - loss: 0.5857 - accuracy: 0.8172\n",
            "Epoch 3/5\n",
            "300/300 [==============================] - 368s 1s/step - loss: 0.5407 - accuracy: 0.8304\n",
            "Epoch 4/5\n",
            "300/300 [==============================] - 367s 1s/step - loss: 0.5290 - accuracy: 0.8306\n",
            "Epoch 5/5\n",
            "300/300 [==============================] - 368s 1s/step - loss: 0.5014 - accuracy: 0.8382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate output"
      ],
      "metadata": {
        "id": "iFcaiVwbLJUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# go through hate speech dection model\n",
        "def hate_speech_preprocess(input_text):\n",
        "  input_text = simplify(input_text)\n",
        "  input_text.replace(r'@\\w+','')\n",
        "  input_text.replace(r'http\\S+','')\n",
        "  tokenizer = TweetTokenizer(preserve_case=True)\n",
        "  input_text = tokenizer.tokenize(input_text)\n",
        "  input_text = remove_stopwords(input_text)\n",
        "  input_text = remove_hashsymbols(input_text)\n",
        "  input_text = rem_shortwords(input_text)\n",
        "  input_text = tokenizer.tokenize(input_text)\n",
        "  input_text = rem_digits(input_text)\n",
        "  input_text = tokenizer.tokenize(input_text)\n",
        "  input_text = rem_nonalpha(input_text)\n",
        "  input_text = ' '.join(input_text)\n",
        "  input_text = vectorizer.transform([input_text])\n",
        "  input_text_hate_speech = input_text\n",
        "  return input_text_hate_speech\n",
        "input_text_hate_speech = hate_speech_preprocess(input_text)\n",
        "hate_speech_classified_output = hate_speech_classifier.predict(input_text_hate_speech)\n",
        "if hate_speech_classified_output == 0:\n",
        "    print('This is a hate speech')\n",
        "elif hate_speech_classified_output == 1:\n",
        "    print('This is an offensive language')\n",
        "elif hate_speech_classified_output==2:\n",
        "    print('This is not a hate speech or offensive language')\n",
        "else:\n",
        "    print(\"error in hate speech detection\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djFLecTMFJSo",
        "outputId": "55f772dd-74ac-4314-fee5-6cd9941711a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an offensive language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# go through the text classification model\n",
        "def text_classification_preprocess(input_text):\n",
        "    input_text = simplify(input_text)\n",
        "    max_len = 60\n",
        "    tokenizer_x_train = tokenizer(\n",
        "            text = input_text,\n",
        "            padding='max_length',\n",
        "            max_length=max_len,\n",
        "            truncation=True,\n",
        "            return_tensors='np',\n",
        "            is_split_into_words=False,\n",
        "            stride = 0,\n",
        "            return_overflowing_tokens=False,\n",
        "        )\n",
        "    X_train_tokenized = tokenizer_x_train['input_ids']\n",
        "    return X_train_tokenized\n",
        "\n",
        "input_text_classification_preprocessed = text_classification_preprocess(input_text)\n",
        "y_pred = text_classification_model.predict(input_text_classification_preprocessed)\n",
        "predicted_category = encoder.classes_[np.argmax(y_pred, axis=1)]\n",
        "text_category_output = predicted_category[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWe9ZVgmHtKO",
        "outputId": "7dff8943-9644-4003-a827-73a95d73bb07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 118ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if hate_speech_classified_output == 2:\n",
        "  print('This is not a hate speech or offensive language')\n",
        "elif hate_speech_classified_output == 0:\n",
        "  print('This is a hate speech on ', text_category_output)\n",
        "elif hate_speech_classified_output == 1:\n",
        "  print('This is an offensive language on', text_category_output.lower())\n",
        "else:\n",
        "  print(\"error in hate speech detection\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfAeKUQK3EPG",
        "outputId": "2b1496e1-9327-4b8c-a6fb-1689e48b88d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an offensive language on sports\n"
          ]
        }
      ]
    }
  ]
}